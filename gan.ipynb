{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>matplotlib inline</b> : the output of plotting commands is displayed inline within frontends like the Jupyter notebook, directly below the code cell that produced it. The resulting plots will then also be stored in the notebook document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd.variable import Variable\n",
    "from torchvision import transforms, datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '/home/pushpull/mount/intHdd/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clubs all the transforms provided to it. So, all the transforms in the (transforms.Compose) are applied to the input one by one\n",
    "\n",
    "def load_data():\n",
    "    compose = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((.5,), (.5,))\n",
    "        ])\n",
    "    out_dir = '{}/'.format(DATA_FOLDER)\n",
    "    return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                     [no_data, gan_out_size]\n",
    "\n",
    "                                ||\n",
    "                                ||\n",
    "                                ||\n",
    "                               \\||/\n",
    "                                \\/\n",
    "                       ____________________\n",
    "                      |                    |\n",
    "                      |   LeakyRelu(0.3)   |\n",
    "            hidden0   |                    |   [no_data, gan_out_size] X [gan_out_size, 512]  ->  [no_data, 512]\n",
    "                      |    Dropout(0.2)    |\n",
    "                      |                    |\n",
    "                      |____________________|\n",
    "\n",
    "                                ||\n",
    "                                ||\n",
    "                          [no_data, 512]\n",
    "                                ||\n",
    "                               \\||/\n",
    "                                \\/\n",
    "                       ____________________\n",
    "                      |                    |\n",
    "                      |   LeakyRelu(0.3)   |\n",
    "            hidden1   |                    |   [no_data, 512] X [512, 256]  ->  [no_data, 256]\n",
    "                      |    Dropout(0.2)    |\n",
    "                      |                    |\n",
    "                      |____________________|\n",
    "\n",
    "                                ||\n",
    "                                ||\n",
    "                          [no_data, 256]\n",
    "                                ||\n",
    "                               \\||/\n",
    "                                \\/\n",
    "                       ____________________\n",
    "                      |                    |\n",
    "                      |   LeakyRelu(0.3)   |\n",
    "            hidden2   |                    |   [no_data, 256] X [256, 128]  ->  [no_data, 128]\n",
    "                      |    Dropout(0.2)    |\n",
    "                      |                    |\n",
    "                      |____________________|\n",
    "\n",
    "                                ||\n",
    "                                ||\n",
    "                          [no_data, 128]\n",
    "                                ||\n",
    "                               \\||/\n",
    "                                \\/\n",
    "                       ____________________\n",
    "                      |                    |\n",
    "                      |                    |\n",
    "            out       |      Sigmoid       |   [no_data, 512] X [128, 1]  ->  [no_data, 1]\n",
    "                      |                    |\n",
    "                      |                    |\n",
    "                      |____________________|\n",
    "\n",
    "                                ||\n",
    "                                ||\n",
    "                                ||\n",
    "                               \\||/\n",
    "                                \\/\n",
    "\n",
    "                           [no_data, 1]\n",
    "\n",
    "                     Probability of Data entered\n",
    "                   sampled from same distribution\n",
    "                      \n",
    "\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator Network\n",
    "\n",
    "# torch.nn.module    :    https://pytorch.org/docs/stable/nn.html\n",
    "#                         Base class for all neural network modules\n",
    "#                         Your models should also subclass this class\n",
    "\n",
    "\n",
    "class discriminator(torch.nn.Module):\n",
    "    def __init__(self, parameter):\n",
    "        super(discriminator, self).__init__()\n",
    "        self.y = 1\n",
    "        self.gan_out = parameter.get(\"gan_out\")\n",
    "        self.Network()\n",
    "        \n",
    "    def Network(self):\n",
    "            self.hidden0 = nn.Sequential( \n",
    "                nn.Linear(self.gan_out, 512),\n",
    "                nn.LeakyReLU(0.3),\n",
    "                nn.Dropout(0.2)\n",
    "                ).cuda()\n",
    "            self.hidden1 = nn.Sequential(\n",
    "                nn.Linear(512, 256),\n",
    "                nn.LeakyReLU(0.3),\n",
    "                nn.Dropout(0.2)\n",
    "                ).cuda()\n",
    "            self.hidden2 = nn.Sequential(\n",
    "                nn.Linear(256, 128),\n",
    "                nn.LeakyReLU(0.3),\n",
    "                nn.Dropout(0.2)\n",
    "                ).cuda()\n",
    "            self.out = nn.Sequential(\n",
    "                torch.nn.Linear(128, self.y),\n",
    "                torch.nn.Sigmoid()\n",
    "            ).cuda()\n",
    "            \n",
    "    def forward(self, x_):\n",
    "            x_ = self.hidden0(x_)\n",
    "            x_ = self.hidden1(x_)\n",
    "            x_ = self.hidden2(x_)\n",
    "            x_ = self.out(x_)\n",
    "            return x_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                       [no_data, gan_in_size]\n",
    "\n",
    "                                ||\n",
    "                                ||\n",
    "                                ||\n",
    "                               \\||/\n",
    "                                \\/\n",
    "                       ____________________\n",
    "                      |                    |\n",
    "                      |                    |\n",
    "            hidden0   |   LeakyRelu(0.3)   |   [no_data, gan_in_size] X [gan_in_size, 128]  ->  [no_data, 128]\n",
    "                      |                    |\n",
    "                      |                    |\n",
    "                      |____________________|\n",
    "\n",
    "                                ||\n",
    "                                ||\n",
    "                          [no_data, 512]\n",
    "                                ||\n",
    "                               \\||/\n",
    "                                \\/\n",
    "                       ____________________\n",
    "                      |                    |\n",
    "                      |                    |\n",
    "            hidden1   |   LeakyRelu(0.3)   |   [no_data, 128] X [128, 256]  ->  [no_data, 256]\n",
    "                      |                    |\n",
    "                      |                    |\n",
    "                      |____________________|\n",
    "\n",
    "                                ||\n",
    "                                ||\n",
    "                          [no_data, 256]\n",
    "                                ||\n",
    "                               \\||/\n",
    "                                \\/\n",
    "                       ____________________\n",
    "                      |                    |\n",
    "                      |                    |\n",
    "            hidden2   |   LeakyRelu(0.3)   |   [no_data, 256] X [256, 512]  ->  [no_data, 512]\n",
    "                      |                    |\n",
    "                      |                    |\n",
    "                      |____________________|\n",
    "\n",
    "                                ||\n",
    "                                ||\n",
    "                          [no_data, 128]\n",
    "                                ||\n",
    "                               \\||/\n",
    "                                \\/\n",
    "                       ____________________\n",
    "                      |                    |\n",
    "                      |                    |\n",
    "            out       |      Sigmoid       |   [no_data, 512] X [512, gan_out_size]  ->  [no_data, gan_out_size]\n",
    "                      |                    |\n",
    "                      |                    |\n",
    "                      |____________________|\n",
    "\n",
    "                                ||\n",
    "                                ||\n",
    "                                ||\n",
    "                               \\||/\n",
    "                                \\/\n",
    "\n",
    "                      [no_data, gan_out_size]\n",
    "\n",
    "                     Probability of Data entered\n",
    "                   sampled from same distribution\n",
    "                      \n",
    "\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Network\n",
    "\n",
    "\n",
    "class generator(torch.nn.Module):\n",
    "    def __init__(self, parameter):\n",
    "        super(generator, self).__init__()\n",
    "        self.gan_out = parameter.get(\"gan_out\")\n",
    "        self.gan_in  = parameter.get(\"gan_in\")\n",
    "        self.Network()\n",
    "        \n",
    "    def Network(self):\n",
    "            self.hidden0 = nn.Sequential(\n",
    "                nn.Linear(self.gan_in, 128),\n",
    "                nn.LeakyReLU(0.3)\n",
    "            ).cuda()\n",
    "            self.hidden1 = nn.Sequential(            \n",
    "                nn.Linear(128, 256),\n",
    "                nn.LeakyReLU(0.3)\n",
    "            ).cuda()\n",
    "            self.hidden2 = nn.Sequential(\n",
    "                nn.Linear(256, 512),\n",
    "                nn.LeakyReLU(0.3)\n",
    "            ).cuda()\n",
    "        \n",
    "            self.out = nn.Sequential(\n",
    "                nn.Linear(512, self.gan_out),\n",
    "                nn.Tanh()\n",
    "            ).cuda()\n",
    "            \n",
    "    def forward(self, x_):\n",
    "            x_ = self.hidden0(x_)\n",
    "            x_ = self.hidden1(x_)\n",
    "            x_ = self.hidden2(x_)\n",
    "            x_ = self.out(x_)\n",
    "            return x_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise : Sample random numbers from normal distribution\n",
    "\n",
    "def noise(length, size):\n",
    "    noise = Variable(torch.randn(length, size)).cuda()\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train discriminator network\n",
    "\n",
    "def train_discriminator(discriminator, no_data, true_data, fake_data, gan_in, d_optimizer, loss):\n",
    "#   Manually set gradients of optimizer of discriminator network to zero\n",
    "    d_optimizer.zero_grad()\n",
    "    \n",
    "#   Feed forward in discriminator network using true data\n",
    "    prob_true  = discriminator(true_data)\n",
    "    true_out   = Variable(torch.ones(no_data, 1)).cuda()\n",
    "#   Get error that how well discriminator network classifies true data\n",
    "    error_true = loss(prob_true, true_out)\n",
    "#   Back propogate in discriminator network to get gradients to update parameter\n",
    "    error_true.backward()\n",
    "\n",
    "#   Feed forward in discriminator network using fake data\n",
    "    prob_false = discriminator(fake_data)\n",
    "    false_out = Variable(torch.zeros(no_data, 1)).cuda()\n",
    "#   Get error that how well discriminator network classifies false data\n",
    "    error_false = loss(prob_false, false_out)\n",
    "#   Back propogate in discriminator network to get gradients to update parameter\n",
    "    error_false.backward()\n",
    "    \n",
    "#   Perform back propogation step\n",
    "    d_optimizer.step()\n",
    "    \n",
    "    return (error_true + error_false)\n",
    "    \n",
    "    \n",
    "def train_generator(generator, discriminator, no_data, gen_data, gan_in, g_optimizer, loss):\n",
    "#   Manually set gradients of optimizer of generator network to zero\n",
    "    g_optimizer.zero_grad()\n",
    "\n",
    "#   Probability that generator had sampled data from same distribution as training data\n",
    "    prob  = discriminator(gen_data)\n",
    "    true_out   = Variable(torch.ones(no_data, 1)).cuda()\n",
    "#   Get error that how well generator samples data from same distribution as training data to be discriminated from discriminator\n",
    "    error = loss(prob, true_out)\n",
    "    \n",
    "#   Back propogate in gradient network to get gradients to update parameter\n",
    "    error.backward()\n",
    "    \n",
    "#   Perform back propogation step\n",
    "    g_optimizer.step()\n",
    "    \n",
    "    return error\n",
    "    \n",
    "    \n",
    "def sample(generator, sample_size, in_size, step):\n",
    "#   Sample input data\n",
    "    noise_input = noise(sample_size, in_size).cuda()\n",
    "#   Generate pseudo data\n",
    "    output      = generator(noise_input).cpu().detach()\n",
    "    plot_sample(output, sample_size, step)\n",
    "    \n",
    "    \n",
    "def plot_sample(image, sample_size, step):\n",
    "    image      = image.view(image.size(0), 1, 28, 28)\n",
    "    grid = vutils.make_grid(image, nrow=int(np.sqrt(sample_size)), normalize=True, scale_each=True)\n",
    "    fig = plt.figure(figsize=(sample_size, sample_size))\n",
    "    location = \"Step\" + str(step) + \".jpg\"\n",
    "    plt.imsave(location, np.moveaxis(grid.numpy(), 0, -1))\n",
    "    plt.imshow(np.moveaxis(grid.numpy(), 0, -1))\n",
    "    display.display(plt.gcf())\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "def train(parameter):\n",
    "    no_epochs = parameter.get(\"no_epochs\")\n",
    "    gan_in  = parameter.get(\"gan_in\")\n",
    "    gan_out = parameter.get(\"gan_out\")\n",
    "    loss = parameter.get(\"loss\")\n",
    "    x = parameter.get(\"true_data\")\n",
    "    generator = parameter.get(\"generator\")\n",
    "    discriminator = parameter.get(\"discriminator\")\n",
    "    d_optimizer = parameter.get(\"d_optimizer\")\n",
    "    g_optimizer = parameter.get(\"g_optimizer\")\n",
    "    sample_size = parameter.get(\"sample_size\")\n",
    "    no_data = parameter.get(\"batch_size\")\n",
    "    \n",
    "    step = 0\n",
    "    g_error = []\n",
    "    d_error = []\n",
    "    n_error = []\n",
    "    \n",
    "    for epoch_no in range(no_epochs):\n",
    "        for i, (data,_) in enumerate(x):\n",
    "            \n",
    "            true_data = data.view(batch_size, gan_out)\n",
    "            true_data = Variable(true_data).cuda()\n",
    "\n",
    "#           Generate random (fake) data for input in generator network to train discriminator network\n",
    "            fake_in_data = noise(no_data, gan_in)\n",
    "#           Detach fake data tensor from graph so not counting it in gradient descent\n",
    "            fake_data = generator(fake_in_data).detach()\n",
    "#           Train discriminator network\n",
    "            disc_error   = train_discriminator(discriminator, no_data, true_data, fake_data, gan_in, d_optimizer, loss)\n",
    "\n",
    "#           Generate random (fake) data for input in generator network to train generator network\n",
    "            gen_in_data = noise(no_data, gan_in)\n",
    "            gen_data = generator(fake_in_data)\n",
    "#           Train generator network\n",
    "            gen_error    = train_generator(generator, discriminator, no_data, gen_data, gan_in, g_optimizer, loss)\n",
    "            \n",
    "            error        = (gen_error + disc_error)\n",
    "\n",
    "#           Append errors into list to create graph\n",
    "            g_error.append(gen_error)\n",
    "            d_error.append(disc_error)\n",
    "            n_error.append(error)\n",
    "\n",
    "#           Generate data after every 10 steps of iteration\n",
    "            if step%10 == 0:\n",
    "                display.clear_output(True)\n",
    "                print(\"Epoch Number:    \", epoch_no)\n",
    "                print(\"Batch Number:    \", i)\n",
    "            \n",
    "                print(\"Generator Error        :    \", gen_error.item())\n",
    "                print(\"Discriminator Error    :    \", disc_error.item())\n",
    "\n",
    "#               Sample data to generate image from given distribution\n",
    "                sample(generator, sample_size, gan_in, step)\n",
    "\n",
    "#               Plot error graph\n",
    "                plt.plot(g_error)\n",
    "                plt.show()\n",
    "                plt.plot(d_error)\n",
    "                plt.show()\n",
    "                plt.plot(n_error)\n",
    "                plt.show()\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch_size and input size and output size of gan\n",
    "\n",
    "batch_size   = 1000\n",
    "gan_in       = 100\n",
    "gan_out      = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_parameter    = {\"gan_out\":gan_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = discriminator(disc_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_parameter    = {\"gan_out\":gan_out,\n",
    "                    \"gan_in\":gan_in}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generator(gen_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_epochs = 200\n",
    "loss = nn.BCELoss()\n",
    "true_data = x\n",
    "d_optimizer = optim.Adam(disc.parameters(), lr=0.0002)\n",
    "g_optimizer = optim.Adam(gen.parameters(), lr=0.0002)\n",
    "sample_size = 9\n",
    "\n",
    "train_parameter = {\"batch_size\":batch_size,\n",
    "                   \"no_epochs\":no_epochs,\n",
    "                   \"gan_in\":gan_in,\n",
    "                   \"gan_out\":gan_out,\n",
    "                   \"loss\":loss,\n",
    "                   \"true_data\":x,\n",
    "                   \"generator\":gen,\n",
    "                   \"discriminator\":disc,\n",
    "                   \"d_optimizer\":d_optimizer,\n",
    "                   \"g_optimizer\":g_optimizer,\n",
    "                   \"sample_size\":sample_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
